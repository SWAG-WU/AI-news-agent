ã€AIå‰æ²¿æ—¥æŠ¥ï½œ2026å¹´02æœˆ06æ—¥ã€‘

ğŸ§  æŠ€æœ¯çªç ´
â€¢ Correctness-Optimized Residual Activation Lens (CORAL): Transferrable and Calibration-Aware Inference-Time Steering
ï¼ˆMiranda Muqing Miaoï¼‰
Large language models (LLMs) exhibit persistent miscalibration, especially after instruction tuning and preference alignmentã€‚
[é“¾æ¥](https://arxiv.org/abs/2602.06022v1)

â€¢ Diffusion Model's Generalization Can Be Characterized by Inductive Biases toward a Data-Dependent Ridge Manifold
ï¼ˆYe Heï¼‰
When a diffusion model is not memorizing the training data set, how does it generalize exactly? A quantitative understanding of the distribution it generates would be beneficial to, for example, an as
[é“¾æ¥](https://arxiv.org/abs/2602.06021v1)

â€¢ A Systematic Evaluation of Large Language Models for PTSD Severity Estimation: The Role of Contextual Knowledge and Modeling Strategies
ï¼ˆPanagiotis Kaliosisï¼‰
Large language models (LLMs) are increasingly being used in a zero-shot fashion to assess mental health conditions, yet we have limited knowledge on what factors affect their accuracyã€‚
[é“¾æ¥](https://arxiv.org/abs/2602.06015v1)

â€¢ GenArena: How Can We Achieve Human-Aligned Evaluation for Visual Generation Tasks?
ï¼ˆRuihang Liï¼‰
The rapid advancement of visual generation models has outpaced traditional evaluation approaches, necessitating the adoption of Vision-Language Models as surrogate judgesã€‚
[é“¾æ¥](https://arxiv.org/abs/2602.06013v1)

â€¢ AgenticPay: A Multi-Agent LLM Negotiation System for Buyer-Seller Transactions
ï¼ˆXianyang Liuï¼‰
Large language model (LLM)-based agents are increasingly expected to negotiate, coordinate, and transact autonomously, yet existing benchmarks lack principled settings for evaluating language-mediated
[é“¾æ¥](https://arxiv.org/abs/2602.06008v1)

â€¢ Speech Emotion Recognition Leveraging OpenAI's Whisper Representations and Attentive Pooling Methods
ï¼ˆAli Shendabadiï¼‰
Speech Emotion Recognition (SER) research has faced limitations due to the lack of standard and sufficiently large datasetsã€‚
[é“¾æ¥](https://arxiv.org/abs/2602.06000v1)

â€¢ Diamond Maps: Efficient Reward Alignment via Stochastic Flow Maps
ï¼ˆPeter Holderriethï¼‰
Flow and diffusion models produce high-quality samples, but adapting them to user preferences or constraints post-training remains costly and brittle, a challenge commonly called reward alignmentã€‚
[é“¾æ¥](https://arxiv.org/abs/2602.05993v1)

â€¢ DSB: Dynamic Sliding Block Scheduling for Diffusion LLMs
ï¼ˆLizhuo Luoï¼‰
Diffusion large language models (dLLMs) have emerged as a promising alternative for text generation, distinguished by their native support for parallel decodingã€‚
[é“¾æ¥](https://arxiv.org/abs/2602.05992v1)

â€¢ Layer-wise LoRA fine-tuning: a similarity metric approach
ï¼ˆKeith Ando Ogawaï¼‰
Pre-training Large Language Models (LLMs) on web-scale datasets becomes fundamental for advancing general-purpose AIã€‚
[é“¾æ¥](https://arxiv.org/abs/2602.05988v1)

â€¢ RISE-Video: Can Video Generators Decode Implicit World Rules?
ï¼ˆMingxin Liuï¼‰
While generative video models have achieved remarkable visual fidelity, their capacity to internalize and reason over implicit world rules remains a critical yet under-explored frontierã€‚
[é“¾æ¥](https://arxiv.org/abs/2602.05986v1)

ğŸ”¥ ä»Šæ—¥äº®ç‚¹
â€¢ DyTopo: Dynamic Topology Routing for Multi-Agent Reasoning via Semantic Matching
Multi-agent systems built from prompted large language models can improve multi-round reasoning, yet most existing pipelines rely on fixed, trajectory

â€¢ PhysicsAgentABM: Physics-Guided Generative Agent-Based Modeling
Large language model (LLM)-based multi-agent systems enable expressive agent reasoning but are expensive to scale and poorly calibrated for timestep-a

â€¢ Curiosity is Knowledge: Self-Consistent Learning and No-Regret Optimization with Active Inference
Active inference (AIF) unifies exploration and exploitation by minimizing the Expected Free Energy (EFE), balancing epistemic value (information gain)

âœ… æ•°æ®æˆªè‡³ 2026å¹´02æœˆ06æ—¥ | æ¥æºï¼šarXiv / å®˜æ–¹åšå®¢ / é¡¶ä¼šç­‰